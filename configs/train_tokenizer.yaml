data_format: jsonl
n_train_examples: 200000

diff_extractor:
  chunksize: 1000
  n_workers: 8

tokenizer:
  _target_: tokenizers.models.BPE
  dropout: 0.5
  unk_token: '[UNK]'

pre_tokenizer:
  _target_: tokenizers.pre_tokenizers.CharDelimiterSplit
  delimiter: æœˆ

trainer:
  _convert_: all
  _target_: tokenizers.trainers.BpeTrainer
  special_tokens: ['[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]', '\n']

paths:
  input_dir: extracted_data_jsonl
  tokenizer_dir: tokenizer
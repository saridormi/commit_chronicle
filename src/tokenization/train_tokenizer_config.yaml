data_format: jsonl

lexer:
  upper_percentile: 0.95
  sep_token: 月
  chunksize: 32000
  n_workers: 16

tokenizer:
  _target_: tokenizers.models.BPE
  dropout: 0.5
  unk_token: '[UNK]'

pre_tokenizer:
  _target_: tokenizers.pre_tokenizers.CharDelimiterSplit
  delimiter: 月

trainer:
  _convert_: all
  _target_: tokenizers.trainers.BpeTrainer
  special_tokens: ['[UNK]', '[CLS]', '[SEP]', '[PAD]', '[MASK]', '\n']

paths:
  input_dir: test/extracted_data_jsonl
  tokenizer_dir: test/tokenizer
  percentile_dir: test/literals_len
